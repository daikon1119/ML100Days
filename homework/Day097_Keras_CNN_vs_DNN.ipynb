{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "[[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ...\n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ...\n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ...\n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ...\n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ...\n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ...\n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n",
    "epochs = 10 # 訓練的 epochs 數量\n",
    "\n",
    "# 讀取資料並檢視\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# 對 label 進行 one-hot encoding (y_trian 原本是純數字)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "print (x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先我們使用一般的 DNN (MLP) 來訓練\n",
    "由於 DNN 只能輸入一維的資料，我們要先將影像進行攤平，若 (50000, 32, 32, 3) 的影像，攤平後會變成 (50000, 32*32*3) = (50000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# 將資料攤平成一維資料\n",
    "x_train = x_train.reshape(50000, 3072) \n",
    "x_test = x_test.reshape(10000, 3072)\n",
    "\n",
    "# 將資料變為 float32 並標準化\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,841,162\n",
      "Trainable params: 1,841,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 2.1688 - accuracy: 0.2487 - val_loss: 1.8252 - val_accuracy: 0.3543\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.8561 - accuracy: 0.3272 - val_loss: 1.7616 - val_accuracy: 0.3740\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.7814 - accuracy: 0.3590 - val_loss: 1.7411 - val_accuracy: 0.3661\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7352 - accuracy: 0.3757 - val_loss: 1.7185 - val_accuracy: 0.3854\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.6922 - accuracy: 0.3937 - val_loss: 1.6831 - val_accuracy: 0.4080\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.6608 - accuracy: 0.4044 - val_loss: 1.6447 - val_accuracy: 0.4162\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.6434 - accuracy: 0.4150 - val_loss: 1.5827 - val_accuracy: 0.4453\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.6182 - accuracy: 0.4212 - val_loss: 1.5871 - val_accuracy: 0.4281\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.6047 - accuracy: 0.4252 - val_loss: 1.5669 - val_accuracy: 0.4380\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.5835 - accuracy: 0.4369 - val_loss: 1.5855 - val_accuracy: 0.4350\n",
      "Test loss: 1.5855413675308228\n",
      "Test accuracy: 0.4350000023841858\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(3072,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下來我們使用 CNN 來訓練神經網路\n",
    "CNN 的原理非常適合處理影像類的資料，就讓我們來看看，同樣的訓練條件，CNN 是否顯著優於 DNN 呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "uint8\n",
      "float32\n",
      "(50000, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[[[[0.23137255 0.24313726 0.24705882]\n",
      "   [0.16862746 0.18039216 0.1764706 ]\n",
      "   [0.19607843 0.1882353  0.16862746]\n",
      "   ...\n",
      "   [0.61960787 0.5176471  0.42352942]\n",
      "   [0.59607846 0.49019608 0.4       ]\n",
      "   [0.5803922  0.4862745  0.40392157]]\n",
      "\n",
      "  [[0.0627451  0.07843138 0.07843138]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.07058824 0.03137255 0.        ]\n",
      "   ...\n",
      "   [0.48235294 0.34509805 0.21568628]\n",
      "   [0.46666667 0.3254902  0.19607843]\n",
      "   [0.47843137 0.34117648 0.22352941]]\n",
      "\n",
      "  [[0.09803922 0.09411765 0.08235294]\n",
      "   [0.0627451  0.02745098 0.        ]\n",
      "   [0.19215687 0.10588235 0.03137255]\n",
      "   ...\n",
      "   [0.4627451  0.32941177 0.19607843]\n",
      "   [0.47058824 0.32941177 0.19607843]\n",
      "   [0.42745098 0.28627452 0.16470589]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8156863  0.6666667  0.3764706 ]\n",
      "   [0.7882353  0.6        0.13333334]\n",
      "   [0.7764706  0.6313726  0.10196079]\n",
      "   ...\n",
      "   [0.627451   0.52156866 0.27450982]\n",
      "   [0.21960784 0.12156863 0.02745098]\n",
      "   [0.20784314 0.13333334 0.07843138]]\n",
      "\n",
      "  [[0.7058824  0.54509807 0.3764706 ]\n",
      "   [0.6784314  0.48235294 0.16470589]\n",
      "   [0.7294118  0.5647059  0.11764706]\n",
      "   ...\n",
      "   [0.72156864 0.5803922  0.36862746]\n",
      "   [0.38039216 0.24313726 0.13333334]\n",
      "   [0.3254902  0.20784314 0.13333334]]\n",
      "\n",
      "  [[0.69411767 0.5647059  0.45490196]\n",
      "   [0.65882355 0.5058824  0.36862746]\n",
      "   [0.7019608  0.5568628  0.34117648]\n",
      "   ...\n",
      "   [0.84705883 0.72156864 0.54901963]\n",
      "   [0.5921569  0.4627451  0.32941177]\n",
      "   [0.48235294 0.36078432 0.28235295]]]\n",
      "\n",
      "\n",
      " [[[0.6039216  0.69411767 0.73333335]\n",
      "   [0.49411765 0.5372549  0.53333336]\n",
      "   [0.4117647  0.40784314 0.37254903]\n",
      "   ...\n",
      "   [0.35686275 0.37254903 0.2784314 ]\n",
      "   [0.34117648 0.3529412  0.2784314 ]\n",
      "   [0.30980393 0.31764707 0.27450982]]\n",
      "\n",
      "  [[0.54901963 0.627451   0.6627451 ]\n",
      "   [0.5686275  0.6        0.6039216 ]\n",
      "   [0.49019608 0.49019608 0.4627451 ]\n",
      "   ...\n",
      "   [0.3764706  0.3882353  0.30588236]\n",
      "   [0.3019608  0.3137255  0.24313726]\n",
      "   [0.2784314  0.28627452 0.23921569]]\n",
      "\n",
      "  [[0.54901963 0.60784316 0.6431373 ]\n",
      "   [0.54509807 0.57254905 0.58431375]\n",
      "   [0.4509804  0.4509804  0.4392157 ]\n",
      "   ...\n",
      "   [0.30980393 0.32156864 0.2509804 ]\n",
      "   [0.26666668 0.27450982 0.21568628]\n",
      "   [0.2627451  0.27058825 0.21568628]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6862745  0.654902   0.6509804 ]\n",
      "   [0.6117647  0.6039216  0.627451  ]\n",
      "   [0.6039216  0.627451   0.6666667 ]\n",
      "   ...\n",
      "   [0.16470589 0.13333334 0.14117648]\n",
      "   [0.23921569 0.20784314 0.22352941]\n",
      "   [0.3647059  0.3254902  0.35686275]]\n",
      "\n",
      "  [[0.64705884 0.6039216  0.5019608 ]\n",
      "   [0.6117647  0.59607846 0.50980395]\n",
      "   [0.62352943 0.6313726  0.5568628 ]\n",
      "   ...\n",
      "   [0.40392157 0.3647059  0.3764706 ]\n",
      "   [0.48235294 0.44705883 0.47058824]\n",
      "   [0.5137255  0.4745098  0.5137255 ]]\n",
      "\n",
      "  [[0.6392157  0.5803922  0.47058824]\n",
      "   [0.61960787 0.5803922  0.47843137]\n",
      "   [0.6392157  0.6117647  0.52156866]\n",
      "   ...\n",
      "   [0.56078434 0.52156866 0.54509807]\n",
      "   [0.56078434 0.5254902  0.5568628 ]\n",
      "   [0.56078434 0.52156866 0.5647059 ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [0.99215686 0.99215686 0.99215686]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.44313726 0.47058824 0.4392157 ]\n",
      "   [0.43529412 0.4627451  0.43529412]\n",
      "   [0.4117647  0.4392157  0.41568628]\n",
      "   ...\n",
      "   [0.28235295 0.31764707 0.3137255 ]\n",
      "   [0.28235295 0.3137255  0.30980393]\n",
      "   [0.28235295 0.3137255  0.30980393]]\n",
      "\n",
      "  [[0.43529412 0.4627451  0.43137255]\n",
      "   [0.40784314 0.43529412 0.40784314]\n",
      "   [0.3882353  0.41568628 0.38431373]\n",
      "   ...\n",
      "   [0.26666668 0.29411766 0.28627452]\n",
      "   [0.27450982 0.29803923 0.29411766]\n",
      "   [0.30588236 0.32941177 0.32156864]]\n",
      "\n",
      "  [[0.41568628 0.44313726 0.4117647 ]\n",
      "   [0.3882353  0.41568628 0.38431373]\n",
      "   [0.37254903 0.4        0.36862746]\n",
      "   ...\n",
      "   [0.30588236 0.33333334 0.3254902 ]\n",
      "   [0.30980393 0.33333334 0.3254902 ]\n",
      "   [0.3137255  0.3372549  0.32941177]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.13725491 0.69803923 0.92156863]\n",
      "   [0.15686275 0.6901961  0.9372549 ]\n",
      "   [0.16470589 0.6901961  0.94509804]\n",
      "   ...\n",
      "   [0.3882353  0.69411767 0.85882354]\n",
      "   [0.30980393 0.5764706  0.77254903]\n",
      "   [0.34901962 0.5803922  0.7411765 ]]\n",
      "\n",
      "  [[0.22352941 0.7137255  0.91764706]\n",
      "   [0.17254902 0.72156864 0.98039216]\n",
      "   [0.19607843 0.7176471  0.9411765 ]\n",
      "   ...\n",
      "   [0.6117647  0.7137255  0.78431374]\n",
      "   [0.5529412  0.69411767 0.80784315]\n",
      "   [0.45490196 0.58431375 0.6862745 ]]\n",
      "\n",
      "  [[0.38431373 0.77254903 0.92941177]\n",
      "   [0.2509804  0.7411765  0.9882353 ]\n",
      "   [0.27058825 0.7529412  0.9607843 ]\n",
      "   ...\n",
      "   [0.7372549  0.7647059  0.80784315]\n",
      "   [0.46666667 0.5294118  0.5764706 ]\n",
      "   [0.23921569 0.30980393 0.3529412 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.28627452 0.30980393 0.3019608 ]\n",
      "   [0.20784314 0.24705882 0.26666668]\n",
      "   [0.21176471 0.26666668 0.3137255 ]\n",
      "   ...\n",
      "   [0.06666667 0.15686275 0.2509804 ]\n",
      "   [0.08235294 0.14117648 0.2       ]\n",
      "   [0.12941177 0.1882353  0.19215687]]\n",
      "\n",
      "  [[0.23921569 0.26666668 0.29411766]\n",
      "   [0.21568628 0.27450982 0.3372549 ]\n",
      "   [0.22352941 0.30980393 0.40392157]\n",
      "   ...\n",
      "   [0.09411765 0.1882353  0.28235295]\n",
      "   [0.06666667 0.13725491 0.20784314]\n",
      "   [0.02745098 0.09019608 0.1254902 ]]\n",
      "\n",
      "  [[0.17254902 0.21960784 0.28627452]\n",
      "   [0.18039216 0.25882354 0.34509805]\n",
      "   [0.19215687 0.3019608  0.4117647 ]\n",
      "   ...\n",
      "   [0.10588235 0.20392157 0.3019608 ]\n",
      "   [0.08235294 0.16862746 0.25882354]\n",
      "   [0.04705882 0.12156863 0.19607843]]]\n",
      "\n",
      "\n",
      " [[[0.7411765  0.827451   0.9411765 ]\n",
      "   [0.7294118  0.8156863  0.9254902 ]\n",
      "   [0.7254902  0.8117647  0.92156863]\n",
      "   ...\n",
      "   [0.6862745  0.7647059  0.8784314 ]\n",
      "   [0.6745098  0.7607843  0.87058824]\n",
      "   [0.6627451  0.7607843  0.8627451 ]]\n",
      "\n",
      "  [[0.7607843  0.8235294  0.9372549 ]\n",
      "   [0.7490196  0.8117647  0.9254902 ]\n",
      "   [0.74509805 0.80784315 0.92156863]\n",
      "   ...\n",
      "   [0.6784314  0.7529412  0.8627451 ]\n",
      "   [0.67058825 0.7490196  0.85490197]\n",
      "   [0.654902   0.74509805 0.84705883]]\n",
      "\n",
      "  [[0.8156863  0.85882354 0.95686275]\n",
      "   [0.8039216  0.84705883 0.9411765 ]\n",
      "   [0.8        0.84313726 0.9372549 ]\n",
      "   ...\n",
      "   [0.6862745  0.7490196  0.8509804 ]\n",
      "   [0.6745098  0.74509805 0.84705883]\n",
      "   [0.6627451  0.7490196  0.84313726]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8117647  0.78039217 0.70980394]\n",
      "   [0.79607844 0.7647059  0.6862745 ]\n",
      "   [0.79607844 0.76862746 0.6784314 ]\n",
      "   ...\n",
      "   [0.5294118  0.5176471  0.49803922]\n",
      "   [0.63529414 0.61960787 0.5882353 ]\n",
      "   [0.65882355 0.6392157  0.5921569 ]]\n",
      "\n",
      "  [[0.7764706  0.74509805 0.6666667 ]\n",
      "   [0.7411765  0.70980394 0.62352943]\n",
      "   [0.7058824  0.6745098  0.5764706 ]\n",
      "   ...\n",
      "   [0.69803923 0.67058825 0.627451  ]\n",
      "   [0.6862745  0.6627451  0.6117647 ]\n",
      "   [0.6862745  0.6627451  0.6039216 ]]\n",
      "\n",
      "  [[0.7764706  0.7411765  0.6784314 ]\n",
      "   [0.7411765  0.70980394 0.63529414]\n",
      "   [0.69803923 0.6666667  0.58431375]\n",
      "   ...\n",
      "   [0.7647059  0.72156864 0.6627451 ]\n",
      "   [0.76862746 0.7411765  0.67058825]\n",
      "   [0.7647059  0.74509805 0.67058825]]]\n",
      "\n",
      "\n",
      " [[[0.8980392  0.8980392  0.9372549 ]\n",
      "   [0.9254902  0.92941177 0.96862745]\n",
      "   [0.91764706 0.9254902  0.96862745]\n",
      "   ...\n",
      "   [0.8509804  0.85882354 0.9137255 ]\n",
      "   [0.8666667  0.8745098  0.91764706]\n",
      "   [0.87058824 0.8745098  0.9137255 ]]\n",
      "\n",
      "  [[0.87058824 0.8666667  0.8980392 ]\n",
      "   [0.9372549  0.9372549  0.9764706 ]\n",
      "   [0.9137255  0.91764706 0.9647059 ]\n",
      "   ...\n",
      "   [0.8745098  0.8745098  0.9254902 ]\n",
      "   [0.8901961  0.89411765 0.93333334]\n",
      "   [0.8235294  0.827451   0.8627451 ]]\n",
      "\n",
      "  [[0.8352941  0.80784315 0.827451  ]\n",
      "   [0.91764706 0.9098039  0.9372549 ]\n",
      "   [0.90588236 0.9137255  0.95686275]\n",
      "   ...\n",
      "   [0.8627451  0.8627451  0.9098039 ]\n",
      "   [0.8627451  0.85882354 0.9098039 ]\n",
      "   [0.7921569  0.79607844 0.84313726]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5882353  0.56078434 0.5294118 ]\n",
      "   [0.54901963 0.5294118  0.49803922]\n",
      "   [0.5176471  0.49803922 0.47058824]\n",
      "   ...\n",
      "   [0.8784314  0.87058824 0.85490197]\n",
      "   [0.9019608  0.89411765 0.88235295]\n",
      "   [0.94509804 0.94509804 0.93333334]]\n",
      "\n",
      "  [[0.5372549  0.5176471  0.49411765]\n",
      "   [0.50980395 0.49803922 0.47058824]\n",
      "   [0.49019608 0.4745098  0.4509804 ]\n",
      "   ...\n",
      "   [0.70980394 0.7058824  0.69803923]\n",
      "   [0.7921569  0.7882353  0.7764706 ]\n",
      "   [0.83137256 0.827451   0.8117647 ]]\n",
      "\n",
      "  [[0.47843137 0.46666667 0.44705883]\n",
      "   [0.4627451  0.45490196 0.43137255]\n",
      "   [0.47058824 0.45490196 0.43529412]\n",
      "   ...\n",
      "   [0.7019608  0.69411767 0.6784314 ]\n",
      "   [0.6431373  0.6431373  0.63529414]\n",
      "   [0.6392157  0.6392157  0.6313726 ]]]]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print (x_train.dtype)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print (x_train.dtype)\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print (x_train.shape[0:])\n",
    "print (type(x_train))\n",
    "print (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 64s 163ms/step - loss: 1.7388 - accuracy: 0.3747 - val_loss: 1.3557 - val_accuracy: 0.5086\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 65s 166ms/step - loss: 1.2935 - accuracy: 0.5416 - val_loss: 1.1663 - val_accuracy: 0.5922\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 1.0958 - accuracy: 0.6162 - val_loss: 1.0047 - val_accuracy: 0.6467\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 64s 164ms/step - loss: 0.9655 - accuracy: 0.6641 - val_loss: 0.8946 - val_accuracy: 0.6862\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 62s 159ms/step - loss: 0.8794 - accuracy: 0.6949 - val_loss: 0.9107 - val_accuracy: 0.6863\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 0.8124 - accuracy: 0.7176 - val_loss: 0.7540 - val_accuracy: 0.7412\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 66s 170ms/step - loss: 0.7556 - accuracy: 0.7378 - val_loss: 0.7854 - val_accuracy: 0.7346\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 61s 155ms/step - loss: 0.7162 - accuracy: 0.7545 - val_loss: 0.7316 - val_accuracy: 0.7519\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 0.6863 - accuracy: 0.7634 - val_loss: 0.6824 - val_accuracy: 0.7687\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 0.6707 - accuracy: 0.7690 - val_loss: 0.7218 - val_accuracy: 0.7620\n",
      "Test loss: 0.7217674851417542\n",
      "Test accuracy: 0.7620000243186951\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同樣運算 10 個 epochs，但 CNN 在 test data 的準確率顯著優於 DNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "1. 請試著調整各個超參數，並說明那些超參數對於結果有明顯的影響?\n",
    "2. CNN 與 DNN 哪個模型的參數數量比較多? 造成參數的數量不同的原因在哪?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. DNN較多, CNN可縮小影像大小\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
